{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ocrmypdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfplumber -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import requests\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rakshit_khajuria_resume.pdf'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoice_pdf = 'rakshit_khajuria_resume.pdf'\n",
    "invoice_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rakshit Khajuria\n",
      "Machine Learning Engineer\n",
      "Talab tillo, gole gujral, 180002 jammu, India rakshitraina1234@gmail.com 9149556155\n",
      "14 Aug 2001 https://www.linkedin.com/in/rakshit-khajuria-3627781b8/ https://github.com/Ryzxxl\n",
      "9149556155\n",
      "SUMMARY\n",
      "Skilled in developing and implementing data-driven solutions to real-world problems, utilizing a range of tools and\n",
      "techniques to extract valuable insights from large and complex data sets. Possesses a diverse experience in cleaning\n",
      "and analyzing data, enhancing existing models. Continuously seeking to enhance my skills and broaden my\n",
      "knowledge in the dynamic \u0000eld of data science.\n",
      "SKILLS\n",
      "Tools : Python, Git, GitHub, Jupyter Notebook, Anaconda, VS Code\n",
      "•\n",
      "Packages : Scikit-Learn, NumPy, Pandas, Matplotlib, NLTK, TensorFlow, Flask, Streamlit, Spacy, Text blob, Vader.\n",
      "•\n",
      "Machine Learning: Supervised learning, Unsupervised learning (Clustering algorithms), Decision trees, Random\n",
      "•\n",
      "forests, Neural Networks.\n",
      "NLP :Sentiment analysis, Text classi\u0000cation, Named Entity Recognition, Word Embedding, Topic Modeling, Deep\n",
      "•\n",
      "Learning NLP models (e.g., Transformer, BERT, LSTMs)\n",
      "SOFT SKILLS\n",
      "Time management Problem-solving Leadership Teamwork Presentation skills Con\u0000dence\n",
      "PROFESSIONAL EXPERIENCE\n",
      "RESEARCH INTERN Jul 2022 – present\n",
      "Jammu, India\n",
      "Developed and implemented a sentiment analysis model to extract and classify\n",
      "•\n",
      "emotional context from scraped Reddit data on suicidal discussions.\n",
      "Utilized advanced techniques, including unsupervised pre-trained models (e.g. Vader,\n",
      "•\n",
      "TextBlob) and clustering algorithms (e.g. KMeans, DBScan), to accurately analyze and\n",
      "interpret sentiment.\n",
      "Enhanced model performance through the use of deep learning techniques, such as\n",
      "•\n",
      "LSTM, BiLSTM etc and \u0000ne-tuning with pre-trained word embeddings (e.g. GloVe).\n",
      "Improved model accuracy by applying different BERT models.\n",
      "•\n",
      "MACHINE LEARNING TARINEE Nov 2020 – Aug 2021\n",
      "Became pro\u0000cient in a wide range of machine learning algorithms, including their Bangalore, India\n",
      "•\n",
      "underlying mathematical principles.\n",
      "Hands-on experience in end-to-end machine learning project development, from data\n",
      "•\n",
      "acquisition to model deployment.\n",
      "Demonstrated expertise in using cloud platforms, such as Heroku, Streamlit, and AWS,\n",
      "•\n",
      "to ef\u0000ciently deploy machine learning models and solutions.\n",
      "rakshitraina1234@gmail.com 1 / 2\n",
      "Rakshit Khajuria\n",
      "Machine Learning Engineer\n",
      "Talab tillo, gole gujral, 180002 jammu, India rakshitraina1234@gmail.com 9149556155\n",
      "14 Aug 2001 https://www.linkedin.com/in/rakshit-khajuria-3627781b8/ https://github.com/Ryzxxl\n",
      "9149556155\n",
      "SUMMARY\n",
      "Skilled in developing and implementing data-driven solutions to real-world problems, utilizing a range of tools and\n",
      "techniques to extract valuable insights from large and complex data sets. Possesses a diverse experience in cleaning\n",
      "and analyzing data, enhancing existing models. Continuously seeking to enhance my skills and broaden my\n",
      "knowledge in the dynamic \u0000eld of data science.\n",
      "SKILLS\n",
      "Tools : Python, Git, GitHub, Jupyter Notebook, Anaconda, VS Code\n",
      "•\n",
      "Packages : Scikit-Learn, NumPy, Pandas, Matplotlib, NLTK, TensorFlow, Flask, Streamlit, Spacy, Text blob, Vader.\n",
      "•\n",
      "Machine Learning: Supervised learning, Unsupervised learning (Clustering algorithms), Decision trees, Random\n",
      "•\n",
      "forests, Neural Networks.\n",
      "NLP :Sentiment analysis, Text classi\u0000cation, Named Entity Recognition, Word Embedding, Topic Modeling, Deep\n",
      "•\n",
      "Learning NLP models (e.g., Transformer, BERT, LSTMs)\n",
      "SOFT SKILLS\n",
      "Time management Problem-solving Leadership Teamwork Presentation skills Con\u0000dence\n",
      "PROFESSIONAL EXPERIENCE\n",
      "RESEARCH INTERN Jul 2022 – present\n",
      "Jammu, India\n",
      "Developed and implemented a sentiment analysis model to extract and classify\n",
      "•\n",
      "emotional context from scraped Reddit data on suicidal discussions.\n",
      "Utilized advanced techniques, including unsupervised pre-trained models (e.g. Vader,\n",
      "•\n",
      "TextBlob) and clustering algorithms (e.g. KMeans, DBScan), to accurately analyze and\n",
      "interpret sentiment.\n",
      "Enhanced model performance through the use of deep learning techniques, such as\n",
      "•\n",
      "LSTM, BiLSTM etc and \u0000ne-tuning with pre-trained word embeddings (e.g. GloVe).\n",
      "Improved model accuracy by applying different BERT models.\n",
      "•\n",
      "MACHINE LEARNING TARINEE Nov 2020 – Aug 2021\n",
      "Became pro\u0000cient in a wide range of machine learning algorithms, including their Bangalore, India\n",
      "•\n",
      "underlying mathematical principles.\n",
      "Hands-on experience in end-to-end machine learning project development, from data\n",
      "•\n",
      "acquisition to model deployment.\n",
      "Demonstrated expertise in using cloud platforms, such as Heroku, Streamlit, and AWS,\n",
      "•\n",
      "to ef\u0000ciently deploy machine learning models and solutions.\n",
      "rakshitraina1234@gmail.com 1 / 2PROJECTS\n",
      "Multi Purpose NLP Application, NLPify Nov 2022 – Nov 2022\n",
      "Developed a sophisticated AI platform that extracts various NLP features, including\n",
      "•\n",
      "sentiment analysis, named entity recognition, and text summarization, from app\n",
      "review data.\n",
      "Performed extensive data preprocessing using techniques such as tokenization,\n",
      "•\n",
      "lemmatization, and vectorization to transform the raw data into a machine-readable\n",
      "format.Created\n",
      "a multi-class classi\u0000cation model using logistic regression and a text\n",
      "•\n",
      "vectorization pipeline to predict sentiment, and deployed the platform as a user-\n",
      "friendly web app using the Streamlit framework.\n",
      "Political Spectrum Detection in Media News using with Deep-Learning and Shap Oct 2022 – Oct 2022\n",
      "PythonBuilt\n",
      "a Political Spectrum Detection deep learning sequential model.\n",
      "•\n",
      "Performed NLP based Tokenization, Lemmatization, vectorization and processed data\n",
      "•\n",
      "using NeatText in Machine understandable language.\n",
      "Simpli\u0000ed code library Shap to built a theoretic approach to explain the output of\n",
      "•\n",
      "sequential deep learning model.\n",
      "Diabetes Prediction using Flask Dec 2021 – Jan 2022\n",
      "Conducted in-depth data analysis on the National Institute of Diabetes and Digestive\n",
      "•\n",
      "and Kidney Diseases dataset\n",
      "Built multiple machine learning models, including Random Forest, logistic regression,\n",
      "•\n",
      "KNN, and SVM, for the prediction of various onsets of type 2 diabetes\n",
      "Created a full-stack application using Flask and deployed it on Heroku for public\n",
      "•\n",
      "access: https://diabetes-prediction-app02.herokuapp.com/\n",
      "CERTIFICATES\n",
      "IBM Data Science Specialization Machine Learning and Deep Learning Trainee\n",
      "Ineuron.ai\n",
      "Mathematics for machine learning\n",
      "Data Science\n",
      "Statistics Foundation - LinkedIn\n",
      "Machine Learning with python\n",
      "EDUCATION\n",
      "Electronics and Communication, Engineering SMVD University, Reasi (J&K) Jul 2019 – Jul 2023\n",
      "Jammu, India\n",
      "ORGANISATIONS\n",
      "A.I CircleActively\n",
      "participated in an A.I circle, collaborating with team members on machine learning projects and mentoring\n",
      "•\n",
      "junior members about machine learning and its potential applications\n",
      "INTERESTS\n",
      "Coding | Football | Sketching | Machine Learning | Deep Learning | Mathematics\n",
      "rakshitraina1234@gmail.com 2 / 2\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(invoice_pdf) as pdf:\n",
    "     text=\"\"\n",
    "     pages = pdf.pages\n",
    "     for page in pages:\n",
    "         text += page.extract_text(x_tolerance=2)\n",
    "         print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "seek of closed file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m page \u001b[39min\u001b[39;00m pages:\n\u001b[1;32m----> 2\u001b[0m          text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m page\u001b[39m.\u001b[39;49mextract_text(x_tolerance\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m          \u001b[39mprint\u001b[39m(text)\n",
      "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pdfplumber\\page.py:315\u001b[0m, in \u001b[0;36mPage.extract_text\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_text\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_textmap(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mas_string\n",
      "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pdfplumber\\page.py:302\u001b[0m, in \u001b[0;36mPage._get_textmap\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     defaults\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mlayout_height\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheight})\n\u001b[0;32m    301\u001b[0m full_kwargs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefaults, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[1;32m--> 302\u001b[0m \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39mchars_to_textmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchars, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfull_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pdfplumber\\container.py:50\u001b[0m, in \u001b[0;36mContainer.chars\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchars\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T_obj_list:\n\u001b[1;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobjects\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mchar\u001b[39m\u001b[39m\"\u001b[39m, [])\n",
      "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pdfplumber\\page.py:194\u001b[0m, in \u001b[0;36mPage.objects\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_objects\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objects\n\u001b[1;32m--> 194\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objects: Dict[\u001b[39mstr\u001b[39m, T_obj_list] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_objects()\n\u001b[0;32m    195\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objects\n",
      "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pdfplumber\\page.py:250\u001b[0m, in \u001b[0;36mPage.parse_objects\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_objects\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, T_obj_list]:\n\u001b[0;32m    249\u001b[0m     objects: Dict[\u001b[39mstr\u001b[39m, T_obj_list] \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 250\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_layout_objects(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayout\u001b[39m.\u001b[39m_objs):\n\u001b[0;32m    251\u001b[0m         kind \u001b[39m=\u001b[39m obj[\u001b[39m\"\u001b[39m\u001b[39mobject_type\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    252\u001b[0m         \u001b[39mif\u001b[39;00m kind \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39manno\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pdfplumber\\page.py:140\u001b[0m, in \u001b[0;36mPage.layout\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    134\u001b[0m device \u001b[39m=\u001b[39m PDFPageAggregator(\n\u001b[0;32m    135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpdf\u001b[39m.\u001b[39mrsrcmgr,\n\u001b[0;32m    136\u001b[0m     pageno\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpage_number,\n\u001b[0;32m    137\u001b[0m     laparams\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpdf\u001b[39m.\u001b[39mlaparams,\n\u001b[0;32m    138\u001b[0m )\n\u001b[0;32m    139\u001b[0m interpreter \u001b[39m=\u001b[39m PDFPageInterpreter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpdf\u001b[39m.\u001b[39mrsrcmgr, device)\n\u001b[1;32m--> 140\u001b[0m interpreter\u001b[39m.\u001b[39;49mprocess_page(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpage_obj)\n\u001b[0;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout: LTPage \u001b[39m=\u001b[39m device\u001b[39m.\u001b[39mget_result()\n\u001b[0;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout\n",
      "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pdfminer\\pdfinterp.py:997\u001b[0m, in \u001b[0;36mPDFPageInterpreter.process_page\u001b[1;34m(self, page)\u001b[0m\n\u001b[0;32m    995\u001b[0m     ctm \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39mx0, \u001b[39m-\u001b[39my0)\n\u001b[0;32m    996\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mbegin_page(page, ctm)\n\u001b[1;32m--> 997\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_contents(page\u001b[39m.\u001b[39;49mresources, page\u001b[39m.\u001b[39;49mcontents, ctm\u001b[39m=\u001b[39;49mctm)\n\u001b[0;32m    998\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mend_page(page)\n\u001b[0;32m    999\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pdfminer\\pdfinterp.py:1014\u001b[0m, in \u001b[0;36mPDFPageInterpreter.render_contents\u001b[1;34m(self, resources, streams, ctm)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Render the content streams.\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \n\u001b[0;32m   1009\u001b[0m \u001b[39mThis method may be called recursively.\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m log\u001b[39m.\u001b[39mdebug(\n\u001b[0;32m   1012\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrender_contents: resources=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m, streams=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m, ctm=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, resources, streams, ctm\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_resources(resources)\n\u001b[0;32m   1015\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_state(ctm)\n\u001b[0;32m   1016\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecute(list_value(streams))\n",
      "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pdfminer\\pdfinterp.py:383\u001b[0m, in \u001b[0;36mPDFPageInterpreter.init_resources\u001b[1;34m(self, resources)\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spec, PDFObjRef):\n\u001b[0;32m    382\u001b[0m             objid \u001b[39m=\u001b[39m spec\u001b[39m.\u001b[39mobjid\n\u001b[1;32m--> 383\u001b[0m         spec \u001b[39m=\u001b[39m dict_value(spec)\n\u001b[0;32m    384\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfontmap[fontid] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrsrcmgr\u001b[39m.\u001b[39mget_font(objid, spec)\n\u001b[0;32m    385\u001b[0m \u001b[39melif\u001b[39;00m k \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mColorSpace\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pdfminer\\pdftypes.py:207\u001b[0m, in \u001b[0;36mdict_value\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdict_value\u001b[39m(x: \u001b[39mobject\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[Any, Any]:\n\u001b[1;32m--> 207\u001b[0m     x \u001b[39m=\u001b[39m resolve1(x)\n\u001b[0;32m    208\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    209\u001b[0m         \u001b[39mif\u001b[39;00m settings\u001b[39m.\u001b[39mSTRICT:\n",
      "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pdfminer\\pdftypes.py:118\u001b[0m, in \u001b[0;36mresolve1\u001b[1;34m(x, default)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resolves an object.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \n\u001b[0;32m    114\u001b[0m \u001b[39mIf this is an array or dictionary, it may still contains\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[39msome indirect objects inside.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39misinstance\u001b[39m(x, PDFObjRef):\n\u001b[1;32m--> 118\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mresolve(default\u001b[39m=\u001b[39;49mdefault)\n\u001b[0;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pdfminer\\pdftypes.py:106\u001b[0m, in \u001b[0;36mPDFObjRef.resolve\u001b[1;34m(self, default)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdoc\u001b[39m.\u001b[39;49mgetobj(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobjid)\n\u001b[0;32m    107\u001b[0m \u001b[39mexcept\u001b[39;00m PDFObjectNotFound:\n\u001b[0;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n",
      "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pdfminer\\pdfdocument.py:866\u001b[0m, in \u001b[0;36mPDFDocument.getobj\u001b[1;34m(self, objid)\u001b[0m\n\u001b[0;32m    864\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getobj_objstm(stream, index, objid)\n\u001b[0;32m    865\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 866\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getobj_parse(index, objid)\n\u001b[0;32m    867\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecipher:\n\u001b[0;32m    868\u001b[0m         obj \u001b[39m=\u001b[39m decipher_all(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecipher, objid, genno, obj)\n",
      "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pdfminer\\pdfdocument.py:818\u001b[0m, in \u001b[0;36mPDFDocument._getobj_parse\u001b[1;34m(self, pos, objid)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_getobj_parse\u001b[39m(\u001b[39mself\u001b[39m, pos: \u001b[39mint\u001b[39m, objid: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[0;32m    817\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parser \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 818\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parser\u001b[39m.\u001b[39;49mseek(pos)\n\u001b[0;32m    819\u001b[0m     (_, objid1) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parser\u001b[39m.\u001b[39mnexttoken()  \u001b[39m# objid\u001b[39;00m\n\u001b[0;32m    820\u001b[0m     (_, genno) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parser\u001b[39m.\u001b[39mnexttoken()  \u001b[39m# genno\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pdfminer\\psparser.py:557\u001b[0m, in \u001b[0;36mPSStackParser.seek\u001b[1;34m(self, pos)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mseek\u001b[39m(\u001b[39mself\u001b[39m, pos: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 557\u001b[0m     PSBaseParser\u001b[39m.\u001b[39;49mseek(\u001b[39mself\u001b[39;49m, pos)\n\u001b[0;32m    558\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[0;32m    559\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pdfminer\\psparser.py:220\u001b[0m, in \u001b[0;36mPSBaseParser.seek\u001b[1;34m(self, pos)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Seeks the parser to the given position.\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mseek: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, pos)\n\u001b[1;32m--> 220\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mseek(pos)\n\u001b[0;32m    221\u001b[0m \u001b[39m# reset the status for nextline()\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbufpos \u001b[39m=\u001b[39m pos\n",
      "\u001b[1;31mValueError\u001b[0m: seek of closed file"
     ]
    }
   ],
   "source": [
    "for page in pages:\n",
    "         text += page.extract_text(x_tolerance=2)\n",
    "         print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rakshit Khajuria\\nMachine Learning Engineer\\nTalab tillo, gole gujral, 180002 jammu, India rakshitraina1234@gmail.com 9149556155\\n14 Aug 2001 https://www.linkedin.com/in/rakshit-khajuria-3627781b8/ https://github.com/Ryzxxl\\n9149556155\\nSUMMARY\\nSkilled in developing and implementing data-driven solutions to real-world problems, utilizing a range of tools and\\ntechniques to extract valuable insights from large and complex data sets. Possesses a diverse experience in cleaning\\nand analyzing data, enhancing existing models. Continuously seeking to enhance my skills and broaden my\\nknowledge in the dynamic \\x00eld of data science.\\nSKILLS\\nTools : Python, Git, GitHub, Jupyter Notebook, Anaconda, VS Code\\n•\\nPackages : Scikit-Learn, NumPy, Pandas, Matplotlib, NLTK, TensorFlow, Flask, Streamlit, Spacy, Text blob, Vader.\\n•\\nMachine Learning: Supervised learning, Unsupervised learning (Clustering algorithms), Decision trees, Random\\n•\\nforests, Neural Networks.\\nNLP :Sentiment analysis, Text classi\\x00cation, Named Entity Recognition, Word Embedding, Topic Modeling, Deep\\n•\\nLearning NLP models (e.g., Transformer, BERT, LSTMs)\\nSOFT SKILLS\\nTime management Problem-solving Leadership Teamwork Presentation skills Con\\x00dence\\nPROFESSIONAL EXPERIENCE\\nRESEARCH INTERN Jul 2022 – present\\nJammu, India\\nDeveloped and implemented a sentiment analysis model to extract and classify\\n•\\nemotional context from scraped Reddit data on suicidal discussions.\\nUtilized advanced techniques, including unsupervised pre-trained models (e.g. Vader,\\n•\\nTextBlob) and clustering algorithms (e.g. KMeans, DBScan), to accurately analyze and\\ninterpret sentiment.\\nEnhanced model performance through the use of deep learning techniques, such as\\n•\\nLSTM, BiLSTM etc and \\x00ne-tuning with pre-trained word embeddings (e.g. GloVe).\\nImproved model accuracy by applying different BERT models.\\n•\\nMACHINE LEARNING TARINEE Nov 2020 – Aug 2021\\nBecame pro\\x00cient in a wide range of machine learning algorithms, including their Bangalore, India\\n•\\nunderlying mathematical principles.\\nHands-on experience in end-to-end machine learning project development, from data\\n•\\nacquisition to model deployment.\\nDemonstrated expertise in using cloud platforms, such as Heroku, Streamlit, and AWS,\\n•\\nto ef\\x00ciently deploy machine learning models and solutions.\\nrakshitraina1234@gmail.com 1 / 2PROJECTS\\nMulti Purpose NLP Application, NLPify Nov 2022 – Nov 2022\\nDeveloped a sophisticated AI platform that extracts various NLP features, including\\n•\\nsentiment analysis, named entity recognition, and text summarization, from app\\nreview data.\\nPerformed extensive data preprocessing using techniques such as tokenization,\\n•\\nlemmatization, and vectorization to transform the raw data into a machine-readable\\nformat.Created\\na multi-class classi\\x00cation model using logistic regression and a text\\n•\\nvectorization pipeline to predict sentiment, and deployed the platform as a user-\\nfriendly web app using the Streamlit framework.\\nPolitical Spectrum Detection in Media News using with Deep-Learning and Shap Oct 2022 – Oct 2022\\nPythonBuilt\\na Political Spectrum Detection deep learning sequential model.\\n•\\nPerformed NLP based Tokenization, Lemmatization, vectorization and processed data\\n•\\nusing NeatText in Machine understandable language.\\nSimpli\\x00ed code library Shap to built a theoretic approach to explain the output of\\n•\\nsequential deep learning model.\\nDiabetes Prediction using Flask Dec 2021 – Jan 2022\\nConducted in-depth data analysis on the National Institute of Diabetes and Digestive\\n•\\nand Kidney Diseases dataset\\nBuilt multiple machine learning models, including Random Forest, logistic regression,\\n•\\nKNN, and SVM, for the prediction of various onsets of type 2 diabetes\\nCreated a full-stack application using Flask and deployed it on Heroku for public\\n•\\naccess: https://diabetes-prediction-app02.herokuapp.com/\\nCERTIFICATES\\nIBM Data Science Specialization Machine Learning and Deep Learning Trainee\\nIneuron.ai\\nMathematics for machine learning\\nData Science\\nStatistics Foundation - LinkedIn\\nMachine Learning with python\\nEDUCATION\\nElectronics and Communication, Engineering SMVD University, Reasi (J&K) Jul 2019 – Jul 2023\\nJammu, India\\nORGANISATIONS\\nA.I CircleActively\\nparticipated in an A.I circle, collaborating with team members on machine learning projects and mentoring\\n•\\njunior members about machine learning and its potential applications\\nINTERESTS\\nCoding | Football | Sketching | Machine Learning | Deep Learning | Mathematics\\nrakshitraina1234@gmail.com 2 / 2'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d86ab636b363cb32d3f7578512f6396b4a169d210dbf8b4d547e88bd9fd74388"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
